# -*- coding: utf-8 -*-
"""EAS6995_HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112jaognsqvTdkHrAyZJKD6scwzFQc8U0
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms

def load_data(batch_size=64):
    transform = transforms.Compose([transforms.ToTensor()])
    # Download the dataset
    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

    # Normalize data to range [0,1]
    # train_dataset.data = train_dataset.data.float() / 255.0
    # test_dataset.data = test_dataset.data.float() / 255.0

    # Subsampling: 50% from each class
    train_indices = subsample_50_percent_per_class(train_dataset)
    train_subset = Subset(train_dataset, train_indices)

    # DataLoader for batching
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader

# Function to perform subsampling 50% from each class
def subsample_50_percent_per_class(dataset):
    labels = dataset.targets.numpy()  # Extract labels from dataset (as numpy array)
    unique_classes = np.unique(labels)  # Get unique class labels (0-9 for FashionMNIST)
    sampled_indices = []  # List to store selected indices
    for cls in unique_classes:
        cls_indices = np.where(labels == cls)[0]  # Find indices of all samples for class `cls`
        sampled_indices.extend(np.random.choice(cls_indices, len(cls_indices) // 2, replace=False))  # Randomly select 50%
    return sampled_indices

# Forward pass for Fully Connected Layer
def fully_connected_forward(X, W, b):
    """
    Perform forward pass for a fully connected (linear) layer.
    X: Input data
    W: Weight matrix
    b: Bias vector
    """
    Z = X @ W + b  # Compute the linear transformation (X * W + b)
    return Z

# Forward pass for ReLU activation
def relu_forward(Z):
    """
    ReLU activation function forward pass.
    Z: Linear output (input to ReLU)
    """
    A = np.maximum(0, Z)  # Apply ReLU function (element-wise)
    return A

# Forward pass for Softmax activation
def softmax_forward(Z):
    """
    Softmax activation function forward pass.
    Z: Output logits (before softmax)
    """
    exp_z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Apply softmax function (numerical stability)
    output = exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Normalize exp_z to get the softmax output
    return output

# Backward pass for Fully Connected Layer (Linear)
def fully_connected_backward(A, W, dZ):
    """
    NOTE CLARIFICATION HERE; dZ is input instead of Y, Z
    Compute gradients for the fully connected (linear) layer.
    Recall, for example:
        dL/dW2 = A1^T x dL/dz2
        dL/da1 = dL/dz2 x dz2/da1 = dL/dz2 x W2^T
    A: Input data from the prior layer (either initial input X if first layer is prior layer, else activation A) (N x d)
    W: Weight matrix (dxK)
    dZ: Gradient of the loss with respect to Z (from the next layer)
    e.g. if current layer is 2, W is W2, dZ is dZ2, and input is A1 output of first layer
    """
    dW = A.T @ dZ / A.shape[0]  # Compute gradient of weights (A^T * dZ)
    db = np.sum(dZ, axis=0, keepdims=True) / A.shape[0]  # Compute gradient of bias (sum of dZ)
    dA = dZ @ W.T  # Compute gradient of loss with respect to input
    return dW, db, dA

# Backward pass for ReLU activation
def relu_backward(Z, dA):
    """
    Compute the gradient of dL/dz, including the ReLU activation
    Recall, for example, dL/dz2 = dL/dz3 x dz3/da x ReLU'(z2) = dL/da x ReLU'(z2)
    Z: Input to ReLU (before activation)
    dA: Gradient of the loss with respect to activations (from the next layer)
    """
    dZ = dA * (Z > 0)  # Compute dZ for ReLU (gradient is 0 for Z <= 0 and dA for Z > 0)
    return dZ

# Backward pass for Softmax Layer
def softmax_backward(S, Y):
    """
    NOTE THE CORRECTION/EFFICIENCY GAIN HERE in using softmax output instead of Z
    Compute the gradient of the loss with respect to softmax output.
    S: Output of softmax
    Y: True labels (one-hot encoded)
    """
    dZ = S - Y  # Compute dZ for softmax (S - Y)
    return dZ

# Weight update function (gradient descent)
def update_weights(weights, biases, grads_W, grads_b, learning_rate=0.01):
    """
    Update weights and biases using gradient descent
    weights: Current weights
    biases: Current biases
    grads_W: Gradient of the weights
    grads_b: Gradient of the biases
    learning_rate: Learning rate for gradient descent
    """
    weights -= learning_rate * grads_W
    biases -= learning_rate * grads_b
    return weights, biases

def evaluate(test_loader, W1, b1, W2, b2, W3, b3):
    """
    Evaluate the model on the test dataset
    test_loader: DataLoader for test data
    W1, b1, W2, b2, W3, b3: Model parameters
    Returns: Test loss and test accuracy
    """
    total_loss = 0
    correct = 0
    total = 0
    output_dim = 10
    for X_batch, Y_batch in test_loader:
        X = X_batch.view(X_batch.shape[0], -1).numpy()
        Y = torch.eye(output_dim)[Y_batch].numpy()

        # Forward pass
        Z1 = fully_connected_forward(X, W1, b1)
        A1 = relu_forward(Z1)
        Z2 = fully_connected_forward(A1, W2, b2)
        A2 = relu_forward(Z2)
        Z3 = fully_connected_forward(A2, W3, b3)
        Y_pred = softmax_forward(Z3)

        # Calculate loss and accuracy
        loss = -np.mean(np.sum(Y * np.log(Y_pred + 1e-8), axis=1))
        total_loss += loss
        correct += np.sum(np.argmax(Y_pred, axis=1) == np.argmax(Y, axis=1))
        total += Y.shape[0]
    return total_loss / len(test_loader), correct / total

# Define the neural network
def train(train_loader, test_loader, epochs=1000, learning_rate=0.01):
    # Initialize weights and biases
    input_dim = 28 * 28  # MNIST image size flattened
    hidden_dim1 = 128    # First hidden layer size (could set differently)
    hidden_dim2 = 64     # Second hidden layer size (could set differently)
    output_dim = 10      # Number of classes in FashionMNIST

    # Initialize weights randomly
    # NOTE THE CORRECTION HERE! I HAD it done using torch but needs to be numpy
    # Note also that this is not using the specific methods I had mentioned for
    # weight initialization (e.g. Xavier or He), this is just random
    W1 = np.random.randn(input_dim, hidden_dim1) * 0.01
    b1 = np.zeros((1, hidden_dim1))
    W2 = np.random.randn(hidden_dim1, hidden_dim2) * 0.01
    b2 = np.zeros((1, hidden_dim2))
    W3 = np.random.randn(hidden_dim2, output_dim) * 0.01
    b3 = np.zeros((1, output_dim))

    # Lists to save training and test metrics
    training_loss = []
    training_accuracy = []
    test_loss = []
    test_accuracy = []

    # Loop through epochs
    for epoch in range(epochs):
        epoch_loss = 0
        correct = 0
        total = 0

        # Iterate through mini-batches
        for X_batch, Y_batch in train_loader:
            # Prepare data
            X = X_batch.view(X_batch.shape[0], -1).numpy()
            Y = torch.eye(output_dim)[Y_batch].numpy()

            # Forward pass
            Z1 = fully_connected_forward(X, W1, b1)
            A1 = relu_forward(Z1)
            Z2 = fully_connected_forward(A1, W2, b2)
            A2 = relu_forward(Z2)
            Z3 = fully_connected_forward(A2, W3, b3)
            Y_pred = softmax_forward(Z3)

            # Loss and accuracy calculation
            loss = -np.mean(np.sum(Y * np.log(Y_pred + 1e-8), axis=1))
            epoch_loss += loss
            correct += np.sum(np.argmax(Y_pred, axis=1) == np.argmax(Y, axis=1))
            total += Y.shape[0]

            # Backward pass
            dZ3 = softmax_backward(Y_pred, Y)
            dW3, db3, dA2 = fully_connected_backward(A2, W3, dZ3)
            dZ2 = relu_backward(Z2, dA2)
            dW2, db2, dA1 = fully_connected_backward(A1, W2, dZ2)
            dZ1 = relu_backward(Z1, dA1)
            dW1, db1, dX = fully_connected_backward(X, W1, dZ1)

            # Update weights
            W1, b1 = update_weights(W1, b1, dW1, db1, learning_rate)
            W2, b2 = update_weights(W2, b2, dW2, db2, learning_rate)
            W3, b3 = update_weights(W3, b3, dW3, db3, learning_rate)

        # Store metrics for this epoch
        avg_loss = epoch_loss / len(train_loader)
        acc = correct / total
        training_loss.append(avg_loss)
        training_accuracy.append(acc)

        # Print progress
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc*100:.2f}%")

        # Evaluate on test set every 100 epochs
        if (epoch + 1) % 100 == 0:
            t_loss, t_acc = evaluate(test_loader, W1, b1, W2, b2, W3, b3)
            test_loss.append(t_loss)
            test_accuracy.append(t_acc)
            print(f"Test Loss: {t_loss:.4f}, Test Accuracy: {t_acc*100:.2f}%")

    print("Training complete!")
    return training_loss, training_accuracy, test_loss, test_accuracy

def main():
    batch_size = 64
    train_loader, test_loader = load_data(batch_size)

    # Start training
    # Note: Adjust epochs and learning rate as needed
    training_loss, training_accuracy, test_loss, test_accuracy = train(train_loader, test_loader, epochs=1000, learning_rate=0.01)

    # Plotting
    epochs_train = np.arange(1, len(training_loss)+1)
    epochs_test = np.arange(100, len(test_loss)*100 +1, 100)

    # Create a figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

    # Plot Training and Test Loss on the first subplot
    ax1.plot(epochs_train, training_loss, label='Training Loss', color='blue', marker='o')
    ax1.plot(epochs_test, test_loss, label='Test Loss', color='red', marker='x')
    ax1.set_title('Loss vs Epoch')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()

    # Plot Training and Test Accuracy on the second subplot
    ax2.plot(epochs_train, training_accuracy, label='Training Accuracy', color='blue', marker='o')
    ax2.plot(epochs_test, test_accuracy, label='Test Accuracy', color='red', marker='x')
    ax2.set_title('Accuracy vs Epoch')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()